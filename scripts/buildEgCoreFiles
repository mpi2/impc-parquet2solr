#!/bin/bash

export YARN_CONF_DIR=~/spark-2.4.5-bin-hadoop2.7/yarn-conf-hh/

# This script builds the solr [unmerged] index files from the parquet files on the hadoop cluster.

# Typical usage:
#   buildEgCoreFiles spark_eg essentialgenes eg/parquet eg/solr
#   buildEgCoreFiles spark_eg ortholog_mapping eg/parquet eg/solr

if [[ $# -lt 4 ]]; then
  echo "Usage: $0 spark_app_name core parquet_dir_head output_dir_head"
  exit 1
fi

# These are the args to parquet2solr-0.0.1-SNAPSHOT.jar. 'limit' is not required.
# args[0] - Spark application name
# args[1] - Path to parquet directory
# args[2] - core name
# args[3] - inferSchema (true or false) set to true to infer the schema; false to use the provided schema.xml
# args[4] - local (true or false) - set to true if running local; false if running on the cluster
# args[5] - Path to output directory
# args[6] - limit

JAR=~/jars/parquet2solr-0.0.1-SNAPSHOT.jar
SPARK_APP_NAME=$1
CORE=$2
PARQUET_DIR=$3/${CORE}
OUTPUT_DIR=$4/${CORE}
COMMAND="$JAR $SPARK_APP_NAME $PARQUET_DIR $CORE true false $OUTPUT_DIR"
echo "COMMAND=$COMMAND"

~/spark-2.4.5-bin-hadoop2.7/bin/spark-submit \
 --driver-memory=32g \
 --master=yarn \
 --conf=spark.yarn.maxAppAttempts=2 \
 --conf=spark.executor.memoryOverhead=5 \
 --conf=spark.yarn.exclude.nodes=hh-hdp-007.ebi.ac.uk:8041 \
 --conf=spark.blacklist.enabled=true \
 --conf=spark.executorEnv.PYSPARK_PYTHON=python36 \
 --conf=spark.yarn.appMasterEnv.PYSPARK_PYTHON=python36 \
 --conf=spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON=python36 \
 --conf=spark.sql.session.timeZone=UTC \
 --deploy-mode=cluster \
 --executor-memory=32G \
 --num-executors=17 \
 --executor-cores=5 \
  $COMMAND
